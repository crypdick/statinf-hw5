---
title: "Homework 5"
author: "Statistical Inference 1"
date: "11/22/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
require(ggplot2)
```


# Guidelines

+ This is a group project. You may work in groups with up to 6 people. You only need to turn in one homework assignment for each group, but make sure that everyone’s name is listed on the assignment. 
+ Also, even if you don’t write the code for every part of the assignment, you should practice the skills in each section.
+ This assignment focuses on simulating data that violates various assumptions of the linear regression model.
+ Some sample code has been included, but you will need to try many different starting values for the parameters, sample size, and distributional assumptions. The included code is just to give you a starting point; it should not be considered sufficient to answer all the questions in each part. (And you are welcome to ignore the sample code and use your own)

# Questions

1.	Nonlinear relationship
    a.	Simulate data for a variety of different non-linear relationships (e.g. polynomial, exponential, sinusoidal).
    b.	Try simulations with a small sample size (e.g. 20), a medium sample size (e.g. n= 100), and a large sample size (e.g. n = 5000). 
    c.	For each simulation, 
        i.	Predict y-hat at several different locations using a confidence interval.
        ii.	Predict the beta coefficients for a linear model using a confidence interval. 
        iii. Find the MSE (to estimate sigma^2)
        iv. Test to see whether the beta(s) are significant
    d. Which of the above tasks were affected by the nonlinear relationship?
    e. After you have experimented with the effects of different model structures, true parameter values, and sample sizes, let's repeat the simulation but test yourself to see whether you can detect non-linearity.
        i. Have R randomly choose whether to simulate data from a true linear model or a true nonlinear model.
        ii. Simulate data accordingly and display informal/ formal diagnostics as appropriate.
        iii. Based on the diagnostics, predict whether the problem areas you mentioned in part d will be affected or not. (Note: You are not predicting whether the assumptions are violated-- just whether they are violated to such an extent that your ability to use the model is compromised)

__Aside:__ Many of the issues you end up facing with a nonlinear relationship can also be seen if an important predictor is excluded from the model. If you have extra time, feel free to play with this issue as well (optional).


```{r}
set.seed(1)
# Sample code to get started
n_small <- 20
n_med <- 100
n_large <- 5000
```


## **POLYNOMIAL** 
```{r}
# TODO: randomize these for part e
b0 <- 10
b1 <- 2

# small sample size
x_smallpoly <- runif(n_small, 0, 10)
eps_smallpoly <- 3 * rnorm(n_small)
y_smallpoly <- b0 - b1 * (x_smallpoly - 5)^2 + eps_smallpoly


# med sample size
x_medpoly <- runif(n_med, 0, 10)
eps_medpoly <- 3 * rnorm(n_med)
y_medpoly <- b0 - b1 * (x_medpoly - 5)^2 + eps_medpoly

# lrg sample size
x_largepoly <- runif(n_large, 0, 10)
eps_largepoly <- 3 * rnorm(n_large)
y_largepoly <- b0 - b1 * (x_largepoly - 5)^2 + eps_largepoly

# part c, for each simulation:
# predict y-hat at several different locations using a confidence interval.
locations <- c(-100, -20, 0, 20, 100)
locations_smallpoly <- data.frame(x_smallpoly = locations)

lm_smallpoly <- lm(y_smallpoly ~ x_smallpoly)
yhat_smallpoly <- predict.lm(lm_smallpoly, newdata=locations_smallpoly, interval="confidence") # predict y_hat

summary(lm_smallpoly)$coef  # model coefficients
(confint(lm_smallpoly))
(anova(lm_smallpoly)$'Mean Sq'[2]) # MSE
(summary(lm_smallpoly)$coeff[,4])  # p-vals of our coeffs
```
Neither of the betas are significant, since they both have p values > 0.05.

```{r}
locations_medpoly <- data.frame(x_medpoly = locations)
lm_medpoly <- lm(y_medpoly ~ x_medpoly)
yhat_medpoly <- predict.lm(lm_medpoly, newdata=locations_medpoly, interval="confidence")
(yhat_medpoly)
summary(lm_medpoly)$coef
(confint(lm_medpoly))
(anova(lm_medpoly)$'Mean Sq'[2])
(summary(lm_medpoly)$coeff[,4])
```
Again, neither of the betas are significant, since they both have p values > 0.05.


```{r}
locations_largepoly <- data.frame(x_largepoly = locations)
lm_largepoly <- lm(y_largepoly ~ x_largepoly)
yhat_largepoly <- predict.lm(lm_largepoly, newdata=locations_largepoly, interval="confidence")
```

```{r}
summary(lm_largepoly)$coef
(confint(lm_largepoly))
(anova(lm_largepoly)$'Mean Sq'[2])
(summary(lm_largepoly)$coeff[,4])
```
This time, both coefficients are very significant!


```{r}
yhat_smallpoly <- data.frame(yhat_smallpoly)

ggplot(yhat_smallpoly, aes(x =locations_smallpoly$x_smallpoly, y = fit)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymax = yhat_smallpoly$upr, ymin = yhat_smallpoly$lwr))


yhat_medpoly <- data.frame(yhat_medpoly)

ggplot(yhat_medpoly, aes(x =locations_medpoly$x_med, y = fit)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymax = yhat_medpoly$upr, ymin = yhat_medpoly$lwr))


yhat_largepoly <- data.frame(yhat_largepoly)

ggplot(yhat_largepoly, aes(x =locations_largepoly$x_large, y = fit)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymax = yhat_largepoly$upr, ymin = yhat_largepoly$lwr))
```
Changing the sample size changed:
The confidence intervals become tighter. TODO expand.

## **EXPONENTIAL**
```{r}
# TODO: randomize these for part e
b0 <- 10
b1 <- 2

# small sample size
x_smallexp <- runif(n_small, 0, 10)
eps_smallexp <- 3 * rnorm(n_small)
y_smallexp <- b0 - b1 * exp(x_smallexp)^2 + eps_smallexp


# med sample size
x_medexp <- runif(n_med, 0, 10)
eps_medexp <- 3 * rnorm(n_med)
y_medexp <- b0 - b1 * (x_medexp - 5)^2 + eps_medexp

# lrg sample size
x_largeexp <- runif(n_large, 0, 10)
eps_largeexp <- 3 * rnorm(n_large)
y_largeexp <- b0 - b1 * (x_largeexp - 5)^2 + eps_largeexp

# part c, for each simulation:
# predict y-hat at several different locations using a confidence interval.
locations <- c(-100, -20, 0, 20, 100)
locations_smallexp <- data.frame(x_smallexp = locations)

lm_smallexp <- lm(y_smallexp ~ x_smallexp)
yhat_smallexp <- predict.lm(lm_smallexp, newdata=locations_smallexp, interval="confidence") # predict y_hat

summary(lm_smallexp)$coef  # model coefficients
(confint(lm_smallexp))
(anova(lm_smallexp)$'Mean Sq'[2]) # MSE
(summary(lm_smallexp)$coeff[,4])  # p-vals of our coeffs
```
TODO beta p-vals

```{r}
locations_medexp <- data.frame(x_medexp = locations)
lm_medexp <- lm(y_medexp ~ x_medexp)
yhat_medexp <- predict.lm(lm_medexp, newdata=locations_medexp, interval="confidence")
(yhat_medexp)
summary(lm_medexp)$coef
(confint(lm_medexp))
(anova(lm_medexp)$'Mean Sq'[2])
(summary(lm_medexp)$coeff[,4])
```
TODO beta p-vals


```{r}
locations_largeexp <- data.frame(x_largeexp = locations)
lm_largeexp <- lm(y_largeexp ~ x_largeexp)
yhat_largeexp <- predict.lm(lm_largeexp, newdata=locations_largeexp, interval="confidence")
```

```{r}
summary(lm_largeexp)$coef
(confint(lm_largeexp))
(anova(lm_largeexp)$'Mean Sq'[2])
(summary(lm_largeexp)$coeff[,4])
```
TODO coeff significance


```{r}
yhat_smallexp <- data.frame(yhat_smallexp)

ggplot(yhat_smallexp, aes(x =locations_smallexp$x_smallexp, y = fit)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymax = yhat_smallexp$upr, ymin = yhat_smallexp$lwr))


yhat_medexp <- data.frame(yhat_medexp)

ggplot(yhat_medexp, aes(x =locations_medexp$x_med, y = fit)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymax = yhat_medexp$upr, ymin = yhat_medexp$lwr))


yhat_largeexp <- data.frame(yhat_largeexp)

ggplot(yhat_largeexp, aes(x =locations_largeexp$x_large, y = fit)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymax = yhat_largeexp$upr, ymin = yhat_largeexp$lwr))
```
Changing the sample size changed:
TODO


## **SINUSOIDAL**


## PART E

2.	Non-normal errors
    a.	Simulate errors from a variety of different non-normal distributions (e.g. gamma, poisson). Make sure to shift the errors over so that they are still centered at 0.
    b.	Try simulations with a small sample size (e.g. 20), a medium sample size (e.g. n= 100), and a large sample size (e.g. n = 5000). 
    c.	For each simulation, 
        i.	Predict y-hat at several different locations using a confidence interval.
        ii.	Predict the beta coefficients for a linear model using a confidence interval. 
        iii. Find the MSE (to estimate sigma^2)
        iv. Test to see whether the beta(s) are significant (t-tests)
    d. Which of the above tasks were affected by the violation of assumptions?
    e. After you have experimented with the effects of different model structures, true parameter values, and sample sizes, let's repeat the simulation but test yourself to see whether you can detect non-normality.
        i. Have R randomly choose whether to simulate data with normal or nonnormal errors
        ii. Simulate data accordingly and display informal/ formal diagnostics as appropriate.
        iii. Based on the diagnostics, predict whether the problem areas you mentioned in part d will be affected or not. (Note: You are not predicting whether the assumptions are violated-- just whether they are violated to such an extent that your ability to use the model is compromised)

## **GAMMA**
```{r}
# Sample code to get started
n <- 200


# TODO randomize this for part e
b0 <- 10
b1 <- 2
eps_alpha <- 2
eps_beta <- 1/4


x <- runif(n, 0, 10)
eps <- (rgamma(n, eps_alpha, 1/eps_beta) - eps_alpha * eps_beta) * 2
y <- b0 - b1 * x + eps

hist(eps)
var(eps)
mean(eps)

```

## **POISSON**
```{r}
#rpois()
```

3. Heterogeneous Variances
    a.	Simulate errors from a variety of different relationships with X (e.g. eps = 2 * sqrt(x))
    b.	Try simulations with a small sample size (e.g. 20), a medium sample size (e.g. n= 100), and a large sample size (e.g. n = 5000). 
    c.	For each simulation, 
        i.	Predict y-hat at several different locations using a confidence interval.
        ii.	Predict the beta coefficients for a linear model using a confidence interval. 
        iii. Find the MSE (to estimate sigma^2-- does that even make sense here?)
        iv. Test to see whether the beta(s) are significant (t-tests)
    d. Which of the above tasks were affected by the violation of assumptions?
    e. After you have experimented with the effects of different model structures, true parameter values, and sample sizes, let's repeat the simulation but test yourself to see whether you can detect heteroskedacity.
        i. Have R randomly choose whether to simulate errors with constant or non-constant variance
        ii. Simulate data accordingly and display informal/ formal diagnostics as appropriate.
        iii. Based on the diagnostics, predict whether the problem areas you mentioned in part d will be affected or not. (Note: You are not predicting whether the assumptions are violated-- just whether they are violated to such an extent that your ability to use the model is compromised)
        
```{r}
# Sample code to get started
n <- 200
b0 <- 10
b1 <- 10

x <- runif(n, 0, 10)
eps <- rnorm(n, sd = 0.5 * x^2)
y <- b0 + b1 * x + eps

```


4. Correlated Errors
    a.	Simulate errors from a variety of different correlation structures.
    b.	Try simulations with a small sample size (e.g. 20), a medium sample size (e.g. n= 100), and a large sample size (e.g. n = 5000). 
    c.	For each simulation, 
        i.	Predict y-hat at several different locations using a confidence interval.
        ii.	Predict the beta coefficients for a linear model using a confidence interval. 
        iii. Find the MSE (to estimate sigma^2)
        iv. Test to see whether the beta(s) are significant (t-tests)
    d. Which of the above tasks were affected by the violation of assumptions?
    e. After you have experimented with the effects of different model structures, true parameter values, and sample sizes, let's repeat the simulation but test yourself to see whether you can detect correlated errors.
        i. Have R randomly choose whether to simulate data with correlated or uncorrelated errors.
        ii. Simulate data accordingly and display informal/ formal diagnostics as appropriate.
        iii. Based on the diagnostics, predict whether the problem areas you mentioned in part d will be affected or not. (Note: You are not predicting whether the assumptions are violated-- just whether they are violated to such an extent that your ability to use the model is compromised)

```{r}
# Sample code to get started
n <- 200
b0 <- 10
b1 <- 10
rho <- 0.9
sigma <- 2

x <- runif(n, 0, 10)

eps <- arima.sim(model = list(ar = rho), n = n)

y <- b0 + b1 * x + eps

# OR...

eps <- rep(0, n)
e.ind <- rnorm(n, mean = 0, sd = (sigma / sqrt(1-rho^2)))
eps[1] <- e.ind[1]
for (i in 2:n) {
  eps[i] <- rho * eps[i-1] + e.ind[i]
}

y <- b0 + b1 * x + eps
```
        
5. Multicollinearity
    a.	Simulate predictors that are correlated with a variety of different correlation structures
    b.	Try simulations with a small sample size (e.g. 20), a medium sample size (e.g. n= 100), and a large sample size (e.g. n = 5000). 
    c.	For each simulation, 
        i.	Predict y-hat at several different locations using a confidence interval.
        ii.	Predict the beta coefficients for a linear model using a confidence interval. 
        iii. Find the MSE (to estimate sigma^2)
        iv. Test to see whether the beta(s) are significant (t-tests)
    d. Which of the above tasks were affected by the violation of assumptions?
    e. After you have experimented with the effects of different model structures, true parameter values, and sample sizes, let's repeat the simulation but test yourself to see whether you can detect collinearity.
        i. Have R randomly choose whether to simulate data with correlated or uncorrelated predictor variables (X).
        ii. Simulate data accordingly and display informal/ formal diagnostics as appropriate.
        iii. Based on the diagnostics, predict whether the problem areas you mentioned in part d will be affected or not. (Note: You are not predicting whether the assumptions are violated-- just whether they are violated to such an extent that your ability to use the model is compromised)

```{r}
n <- 20
b0 <- 10
b1 <- 3
b2 <- 7

simga <- 2

x1 <- runif(n, 0, 10)
x2 <- x1 + rnorm(n)
cor(x1, x2)


eps <- rnorm(n, sd = sigma)
y <- b0 + b1 * x1 + b2 * x2 + eps
```

        
6. Put it all together: Combine the code from the previous 5 parts. Have R randomly choose whether to generate data that violates one (or more) of the assumptions, or whether all the assumptions are valid. Show appropriate diagnostics and test yourself to see if you can predict whether there are problem areas or not. Repeat the simulation several times and record your accuracy at detecting the different problem areas.