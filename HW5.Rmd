---
title: "Homework 5"
author: "Statistical Inference 1"
date: "11/22/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
require(ggplot2)
```


# Guidelines

+ This is a group project. You may work in groups with up to 6 people. You only need to turn in one homework assignment for each group, but make sure that everyone’s name is listed on the assignment. 
+ Also, even if you don’t write the code for every part of the assignment, you should practice the skills in each section.
+ This assignment focuses on simulating data that violates various assumptions of the linear regression model.
+ Some sample code has been included, but you will need to try many different starting values for the parameters, sample size, and distributional assumptions. The included code is just to give you a starting point; it should not be considered sufficient to answer all the questions in each part. (And you are welcome to ignore the sample code and use your own)

```{r run_sim}
check_violations <- function(sample_size, orig_new_obsv, violation_name, violation_func, sigma_sq, orig_X) {
# new_obs used to calculate y
# x, eps used to calculate y_hat
    if (violation_name == "None") {
      X <- orig_X
      new_obsv <- orig_new_obsv
      eps <- rnorm(sample_size, mean = 0, sd = sqrt(sigma_sq))
      return(list(x,new_obsv,eps))
    }
    if (violation_name == "Nonlinear relationship") {
      X <- violation_func(orig_X)
      new_obsv <- violation_func(orig_new_obsv)
      eps <- rnorm(sample_size, mean = 0, sd = sqrt(sigma_sq))
      return(list(X,new_obsv,eps))
    }
    if (violation_name == "Non-normal errors") {
      if (violation_func == "Gamma") {
        X <- orig_X
        new_obsv <- orig_new_obsv
        eps_alpha <- 2
        eps_beta <- 1/4
        eps <- (rgamma(sample_size, shape = eps_alpha, scale = eps_beta) - eps_alpha * eps_beta) * 2
        return(list(X,new_obsv,eps))
      }
      if (violation_func == "Poisson") {
        X <- orig_X
        new_obsv <- orig_new_obsv
        lambda <- 4
        eps <- (rpois(sample_size, lambda) - lambda) * 2
        return(list(X,new_obsv,eps))
      }
    }
    if (violation_name == "Heterogeneous variances"){
      X <- orig_X
      new_obsv <- orig_new_obsv
      eps <- rnorm(sample_size, mean = 0, sd = violation_func(X[,2]))
      return(list(X,new_obsv,eps))
    }
}


# function that will run simulations, violating a specific assumption
run_simulations <- function(num_runs, sample_size, num_new_obsv, violation, violation_func, parameters, sigma_sq) {
  
  sim_results <- data.frame(bad_prediction_rate = 0,
                            bad_b0_rate = 0, 
                            bad_b1_rate = 0, 
                            average_mse = 0, 
                            insignificant_b0_rate = 0, 
                            insignificant_b1_rate = 0)

  # b0 <- parameters[1]
  # b1 <- parameters[2]
  
  for (i in 1:num_runs) {
    X <- runif(sample_size, 0, 10)
    new_obsv <- runif(num_new_obsv, 0, 10)
    
    violated_data <- check_violations(sample_size, new_obsv, violation, violation_func, sigma_sq, X)
    violated_X <- violated_data[[1]]
    violated_new_obsv <- violated_data[[2]]
    eps <- violated_data[[3]]
    
    # add in column of ones to capture beta0 when generating Y and expected value of Y
    Y <- (cbind(1,violated_X) %*% parameters) + eps
    exp_val_Y <- cbind(1,violated_new_obsv) %*% parameters
    
    
    # fit a linear model using the simulated data
    my_lm <- lm(Y ~ X)
    
    # predict y for new values of x using the fitted linear model
    y_hat_ci <- predict(my_lm, newdata = data.frame(x = new_obsv), interval = "confidence")
    # check whether the confidence interval actually captures the expected value of y from the true model
    bad_predictions <- num_new_obsv - sum((exp_val_Y >= y_hat_ci[,2]) & (exp_val_Y <= y_hat_ci[,3]))
    sim_results$bad_prediction_rate <- sim_results$bad_prediction_rate + bad_predictions
    
    # get confidence iterval for beta hat
    beta_ci <- confint(my_lm)
    # check whether the true value for the betas is caputured in the confidence interval
    good_betas <- ((parameters >= beta_ci[,1]) & (parameters <= beta_ci[,2]))
    if (good_betas[1] == FALSE) sim_results$bad_b0_rate <- sim_results$bad_b0_rate + 1
    if (good_betas[2] == FALSE) sim_results$bad_b1_rate <- sim_results$bad_b1_rate + 1
    
    # add the mse to the sum of MSE for all runs (to be averaged at the end)
    sim_results$average_mse <- sim_results$average_mse + (anova(my_lm)$'Mean Sq'[2])
    
    
    # check which beta_hats are significant based on the p-values
    p_vals <- summary(my_lm)$coeff[,4]
    if (p_vals[1] > .05) sim_results$insignificant_b0_rate <- sim_results$insignificant_b0_rate + 1
    if (p_vals[2] > .05) sim_results$insignificant_b1_rate <- sim_results$insignificant_b1_rate + 1
  }
  sim_results$bad_prediction_rate <- sim_results$bad_prediction_rate/(num_new_obsv*num_runs)
  sim_results[,2:6] <- sim_results[,2:6]/num_runs

  payload <- list(sim_results=sim_results, lm_model=my_lm, X=X, Y=Y, eps=eps, y_hat_ci=y_hat_ci, exp_val_Y=exp_val_Y)
  return(payload)
  
}
```



```{r show_diagnostics}
show_diagnostics <- function(payload) {
  # unpack payload into this
  list2env(payload, envir=environment())

  # print out essentials
  # print() required inside function
  print(sim_results)
  print(summary(lm_model))
  print(anova(lm_model))
  
  # plots
  plot(lm_model)
  plot(y ~ x)
abline(lm_model)
#ggplot(yhat_medsine, aes(x =locations_medsine$x_med, y = fit)) +
#geom_point(size = 4) +
#geom_errorbar(aes(ymax = yhat_medsine$upr, ymin = #yhat_medsine$lwr))
}
```

# Questions

1.	Nonlinear relationship
    a.	Simulate data for a variety of different non-linear relationships (e.g. polynomial, exponential, sinusoidal).
    b.	Try simulations with a small sample size (e.g. 20), a medium sample size (e.g. n= 100), and a large sample size (e.g. n = 5000). 
    c.	For each simulation, 
        i.	Predict y-hat at several different locations using a confidence interval.
        ii.	Predict the beta coefficients for a linear model using a confidence interval. 
        iii. Find the MSE (to estimate sigma^2)
        iv. Test to see whether the beta(s) are significant
    d. Which of the above tasks were affected by the nonlinear relationship?
    e. After you have experimented with the effects of different model structures, true parameter values, and sample sizes, let's repeat the simulation but test yourself to see whether you can detect non-linearity.
        i. Have R randomly choose whether to simulate data from a true linear model or a true nonlinear model.
        ii. Simulate data accordingly and display informal/ formal diagnostics as appropriate.
        iii. Based on the diagnostics, predict whether the problem areas you mentioned in part d will be affected or not. (Note: You are not predicting whether the assumptions are violated-- just whether they are violated to such an extent that your ability to use the model is compromised)  
        
```{r}
n_sims <- 100
n_samps <- 100
num_new_obsv <- 10
violation_type <- "Nonlinear relationship"
violation_func <- sin
parameters <- c(0,1)
sigma_sq <- 1

payload <- run_simulations(n_sims, n_samps, num_new_obsv, violation_type, violation_func, parameters, sigma_sq)
#cat(violation_type, n_sims, n_samps, num_new_obsv, str(violation_func), parameters, sigma_sq)
show_diagnostics(payload)
```
        

__Aside:__ Many of the issues you end up facing with a nonlinear relationship can also be seen if an important predictor is excluded from the model. If you have extra time, feel free to play with this issue as well (optional).


```{r}
set.seed(1)
# Sample code to get started
n_small <- 20
n_med <- 100
n_large <- 5000
locations <- c(-10:10)*10
```

```{r non_linear_relationship}
num_runs <- 100
num_new_obsv <- 20
parameters <- c(10,2)
sigma_sq <- 4
```

## **POLYNOMIAL**
```{r polynomial_relationship}
poly_func <- function(x) (x-5)^2
poly_small_results <- run_simulations(num_runs,
                                      n_small, 
                                      num_new_obsv, 
                                      violation = "Nonlinear relationship", 
                                      violation_func = poly_func, 
                                      parameters, 
                                      sigma_sq)

poly_med_results <-run_simulations(num_runs,
                                   n_med, 
                                   num_new_obsv, 
                                   violation =  "Nonlinear relationship", 
                                   violation_func = poly_func, 
                                   parameters, 
                                   sigma_sq)

poly_large_results <-run_simulations(num_runs, 
                                     n_large, 
                                     num_new_obsv, 
                                     violation =  "Nonlinear relationship", 
                                     violation_func = poly_func, 
                                     parameters, 
                                     sigma_sq)
polynomial_results <- rbind(small_samp = poly_small_results, 
                            med_samp = poly_med_results, 
                            large_samp = poly_large_results)
polynomial_results
```
## **EXPONENTIAL**
```{r exponential_relationship}
exp_small_results <- run_simulations(num_runs,
                                     n_small, 
                                     num_new_obsv, 
                                     violation =  "Nonlinear relationship", 
                                     violation_func = exp, 
                                     parameters, 
                                     sigma_sq)

exp_med_results <-run_simulations(num_runs, 
                                  n_med, 
                                  num_new_obsv, 
                                  violation =  "Nonlinear relationship",
                                  violation_func = exp, 
                                  parameters, 
                                  sigma_sq)

exp_large_results <-run_simulations(num_runs,
                                    n_large, 
                                    num_new_obsv, 
                                    violation =  "Nonlinear relationship", 
                                    violation_func = exp, 
                                    parameters, 
                                    sigma_sq)

exponential_results <- rbind(small_samp = exp_small_results,
                             med_samp = exp_med_results, 
                             large_samp = exp_large_results)
exponential_results
```
## **SINUSOIDAL**
```{r sinusoidal_relationship}
sin_small_results <- run_simulations(num_runs, 
                                     n_small, 
                                     num_new_obsv, 
                                     violation =  "Nonlinear relationship", 
                                     violation_func = sin, 
                                     parameters, 
                                     sigma_sq)

sin_med_results <- run_simulations(num_runs, 
                                   n_med, 
                                   num_new_obsv, 
                                   violation =  "Nonlinear relationship", 
                                   violation_func = sin, 
                                   parameters, 
                                   sigma_sq)

sin_large_results <- run_simulations(num_runs, 
                                     n_large, 
                                     num_new_obsv, 
                                     violation =  "Nonlinear relationship", 
                                     violation_func = sin, 
                                     parameters, 
                                     sigma_sq)

sinusoidal_results <- rbind(small_samp = sin_small_results, 
                            med_samp = sin_med_results, 
                            large_samp = sin_large_results)
sinusoidal_results
```
## **LOGARITHMIC**
```{r logarithmic_relationship}
log_small_results <- run_simulations(num_runs, 
                                     n_small, 
                                     num_new_obsv, 
                                     violation =  "Nonlinear relationship", 
                                     violation_func = log,
                                     parameters, 
                                     sigma_sq)

log_med_results <- run_simulations(num_runs, 
                                   n_med, 
                                   num_new_obsv, 
                                   violation =  "Nonlinear relationship", 
                                   violation_func = log, 
                                   parameters, 
                                   sigma_sq)

log_large_results <- run_simulations(num_runs, 
                                     n_large, 
                                     num_new_obsv, 
                                     violation =  "Nonlinear relationship", 
                                     violation_func = log, 
                                     parameters, 
                                     sigma_sq)

logarithmic_results <- rbind(small_samp = log_small_results, 
                             med_samp = log_med_results, 
                             large_samp = log_large_results)
logarithmic_results
```

## **POLYNOMIAL** 
```{r}
# TODO: randomize these for part e
b0 <- 10
b1 <- 2

# small sample size
x_smallpoly <- runif(n_small, 0, 10)
eps_smallpoly <- 3 * rnorm(n_small)
y_smallpoly <- b0 - b1 * (x_smallpoly - 5)^2 + eps_smallpoly


# med sample size
x_medpoly <- runif(n_med, 0, 10)
eps_medpoly <- 3 * rnorm(n_med)
y_medpoly <- b0 - b1 * (x_medpoly - 5)^2 + eps_medpoly

# lrg sample size
x_largepoly <- runif(n_large, 0, 10)
eps_largepoly <- 3 *rnorm(n_large)
y_largepoly <- b0 - b1 * (x_largepoly - 5)^2 + eps_largepoly

y_test <- b0 + b1 *x_largepoly + eps_largepoly
lm_test <- lm(y_test ~ x_largepoly)
anova(lm_test)
# part c, for each simulation:
# predict y-hat at several different locations using a confidence interval
locations_smallpoly <- data.frame(x_smallpoly = locations)

lm_smallpoly <- lm(y_smallpoly ~ x_smallpoly)
yhat_smallpoly <- predict.lm(lm_smallpoly, newdata=locations_smallpoly, interval="confidence") # predict y_hat
(yhat_smallpoly)
summary(lm_smallpoly)$coef  # model coefficients
(confint(lm_smallpoly))
(anova(lm_smallpoly)$'Mean Sq'[2]) # MSE
(summary(lm_smallpoly)$coeff[,4]) # p-vals of our coeffs
```
Neither of the betas are significant, since they both have p values > 0.05.

```{r}
locations_medpoly <- data.frame(x_medpoly = locations)
lm_medpoly <- lm(y_medpoly ~ x_medpoly)
yhat_medpoly <- predict.lm(lm_medpoly, newdata=locations_medpoly, interval="confidence")
(yhat_medpoly)
summary(lm_medpoly)$coef
(confint(lm_medpoly))
(anova(lm_medpoly)$'Mean Sq'[2])
(summary(lm_medpoly)$coeff[,4])
```
Again, neither of the betas are significant, since they both have p values > 0.05.


```{r}
locations_largepoly <- data.frame(x_largepoly = locations)
lm_largepoly <- lm(y_largepoly ~ x_largepoly)
yhat_largepoly <- predict.lm(lm_largepoly, newdata=locations_largepoly, interval="confidence")
```

```{r}
summary(lm_largepoly)$coef
(confint(lm_largepoly))
(anova(lm_largepoly)$'Mean Sq'[2])
(summary(lm_largepoly)$coeff[,4])
```
This time, both coefficients are very significant!


```{r}
yhat_smallpoly <- data.frame(yhat_smallpoly)

ggplot(yhat_smallpoly, aes(x =locations_smallpoly$x_smallpoly, y = fit)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymax = yhat_smallpoly$upr, ymin = yhat_smallpoly$lwr))


yhat_medpoly <- data.frame(yhat_medpoly)

ggplot(yhat_medpoly, aes(x =locations_medpoly$x_med, y = fit)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymax = yhat_medpoly$upr, ymin = yhat_medpoly$lwr))


yhat_largepoly <- data.frame(yhat_largepoly)

ggplot(yhat_largepoly, aes(x =locations_largepoly$x_large, y = fit)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymax = yhat_largepoly$upr, ymin = yhat_largepoly$lwr))
```
Changing the sample size changed:
The confidence intervals become tighter. TODO expand.

## **EXPONENTIAL**
```{r}
# TODO: randomize these for part e
b0 <- 10
b1 <- 2

# small sample size
x_smallexp <- runif(n_small, 0, 10)
eps_smallexp <- 3 * rnorm(n_small)
y_smallexp <- b0 - b1 * exp(x_smallexp) + eps_smallexp


# med sample size
x_medexp <- runif(n_med, 0, 10)
eps_medexp <- 3 * rnorm(n_med)
y_medexp <- b0 - b1 * exp(x_medexp) + eps_medexp

# lrg sample size
x_largeexp <- runif(n_large, 0, 10)
eps_largeexp <- 3 * rnorm(n_large)
y_largeexp <- b0 - b1 * exp(x_largeexp) + eps_largeexp

# part c, for each simulation:
# predict y-hat at several different locations using a confidence interval.
locations_smallexp <- data.frame(x_smallexp = locations)

lm_smallexp <- lm(y_smallexp ~ x_smallexp)
yhat_smallexp <- predict.lm(lm_smallexp, newdata=locations_smallexp, interval="confidence") # predict y_hat

summary(lm_smallexp)$coef  # model coefficients
(confint(lm_smallexp))
(anova(lm_smallexp)$'Mean Sq'[2]) # MSE
(summary(lm_smallexp)$coeff[,4])  # p-vals of our coeffs
```
TODO beta p-vals

```{r}
locations_medexp <- data.frame(x_medexp = locations)
lm_medexp <- lm(y_medexp ~ x_medexp)
yhat_medexp <- predict.lm(lm_medexp, newdata=locations_medexp, interval="confidence")
(yhat_medexp)
summary(lm_medexp)$coef
(confint(lm_medexp))
(anova(lm_medexp)$'Mean Sq'[2])
(summary(lm_medexp)$coeff[,4])
```
TODO beta p-vals


```{r}
locations_largeexp <- data.frame(x_largeexp = locations)
lm_largeexp <- lm(y_largeexp ~ x_largeexp)
yhat_largeexp <- predict.lm(lm_largeexp, newdata=locations_largeexp, interval="confidence")
```

```{r}
summary(lm_largeexp)$coef
(confint(lm_largeexp))
(anova(lm_largeexp)$'Mean Sq'[2])
(summary(lm_largeexp)$coeff[,4])
```
TODO coeff significance


```{r}
yhat_smallexp <- data.frame(yhat_smallexp)

ggplot(yhat_smallexp, aes(x =locations_smallexp$x_smallexp, y = fit)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymax = yhat_smallexp$upr, ymin = yhat_smallexp$lwr))


yhat_medexp <- data.frame(yhat_medexp)

ggplot(yhat_medexp, aes(x =locations_medexp$x_med, y = fit)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymax = yhat_medexp$upr, ymin = yhat_medexp$lwr))


yhat_largeexp <- data.frame(yhat_largeexp)

ggplot(yhat_largeexp, aes(x =locations_largeexp$x_large, y = fit)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymax = yhat_largeexp$upr, ymin = yhat_largeexp$lwr))
```
Changing the sample size changed:
TODO


## **SINUSOIDAL**
```{r}
# TODO: randomize these for part e
b0 <- 0
b1 <- 20*pi

# small sample size
x_smallsine <- runif(n_small, 0, 10)
eps_smallsine <- 3 * rnorm(n_small)
y_smallsine <- b0 + b1 * sin(x_smallsine) + eps_smallsine


# med sample size
x_medsine <- runif(n_med, 0, 10)
eps_medsine <- 3 * rnorm(n_med)
y_medsine <- b0 - b1 * sin(x_medsine) + eps_medsine

# lrg sample size
x_largesine <- runif(n_large, 0, 10)
eps_largesine <- 3 * rnorm(n_large)
y_largesine <- b0 - b1 * sin(x_largesine) + eps_largesine

# part c, for each simulation:
# predict y-hat at several different locations using a confidence interval.
locations_smallsine <- data.frame(x_smallsine = locations)

lm_smallsine <- lm(y_smallsine ~ x_smallsine)
yhat_smallsine <- predict.lm(lm_smallsine, newdata=locations_smallsine, interval="confidence") # predict y_hat

summary(lm_smallsine)$coef  # model coefficients
(confint(lm_smallsine))
(anova(lm_smallsine)$'Mean Sq'[2]) # MSE
(summary(lm_smallsine)$coeff[,4])  # p-vals of our coeffs
```
TODO beta p-vals

```{r}
locations_medsine <- data.frame(x_medsine = locations)
lm_medsine <- lm(y_medsine ~ x_medsine)
yhat_medsine <- predict.lm(lm_medsine, newdata=locations_medsine, interval="confidence")
(yhat_medsine)
summary(lm_medsine)$coef
(confint(lm_medsine))
(anova(lm_medsine)$'Mean Sq'[2])
(summary(lm_medsine)$coeff[,4])
```
TODO beta p-vals


```{r}
locations_largesine <- data.frame(x_largesine = locations)
lm_largesine <- lm(y_largesine ~ x_largesine)
yhat_largesine <- predict.lm(lm_largesine, newdata=locations_largesine, interval="confidence")
```

```{r}
summary(lm_largesine)$coef
(confint(lm_largesine))
(anova(lm_largesine)$'Mean Sq'[2])
(summary(lm_largesine)$coeff[,4])
```
TODO coeff significance


```{r}
yhat_smallsine <- data.frame(yhat_smallsine)

ggplot(yhat_smallsine, aes(x =locations_smallsine$x_smallsine, y = fit)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymax = yhat_smallsine$upr, ymin = yhat_smallsine$lwr))


yhat_medsine <- data.frame(yhat_medsine)

ggplot(yhat_medsine, aes(x =locations_medsine$x_med, y = fit)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymax = yhat_medsine$upr, ymin = yhat_medsine$lwr))


yhat_largesine <- data.frame(yhat_largesine)

ggplot(yhat_largesine, aes(x =locations_largesine$x_large, y = fit)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymax = yhat_largesine$upr, ymin = yhat_largesine$lwr))
```
Changing the sample size changed:
TODO

## PART E


2.	Non-normal errors
    a.	Simulate errors from a variety of different non-normal distributions (e.g. gamma, poisson). Make sure to shift the errors over so that they are still centered at 0.
    b.	Try simulations with a small sample size (e.g. 20), a medium sample size (e.g. n= 100), and a large sample size (e.g. n = 5000). 
    c.	For each simulation, 
        i.	Predict y-hat at several different locations using a confidence interval.
        ii.	Predict the beta coefficients for a linear model using a confidence interval. 
        iii. Find the MSE (to estimate sigma^2)
        iv. Test to see whether the beta(s) are significant (t-tests)
    d. Which of the above tasks were affected by the violation of assumptions?
    e. After you have experimented with the effects of different model structures, true parameter values, and sample sizes, let's repeat the simulation but test yourself to see whether you can detect non-normality.
        i. Have R randomly choose whether to simulate data with normal or nonnormal errors
        ii. Simulate data accordingly and display informal/ formal diagnostics as appropriate.
        iii. Based on the diagnostics, predict whether the problem areas you mentioned in part d will be affected or not. (Note: You are not predicting whether the assumptions are violated-- just whether they are violated to such an extent that your ability to use the model is compromised)

## **GAMMA**
```{r gamma_errors}
gamma_small_results <- run_simulations(num_runs, 
                                     n_small, 
                                     num_new_obsv, 
                                     violation =  "Non-normal errors", 
                                     violation_func = "Gamma",
                                     parameters, 
                                     sigma_sq)

gamma_med_results <- run_simulations(num_runs, 
                                   n_med, 
                                   num_new_obsv, 
                                   violation =  "Non-normal errors", 
                                   violation_func = "Gamma", 
                                   parameters, 
                                   sigma_sq)

gamma_large_results <- run_simulations(num_runs, 
                                     n_large, 
                                     num_new_obsv, 
                                     violation =  "Non-normal errors", 
                                     violation_func = "Gamma", 
                                     parameters, 
                                     sigma_sq)

gamma_results <- rbind(small_samp = gamma_small_results, 
                             med_samp = gamma_med_results, 
                             large_samp = gamma_large_results)
gamma_results
```

```{r}
# Sample code to get started
n <- n_large


# TODO randomize this for part e
b0 <- 10
b1 <- 2
eps_alpha <- 2
eps_beta <- 1/4


x <- runif(n, 0, 10)
eps <- (rgamma(n, eps_alpha, 1/eps_beta) - eps_alpha * eps_beta) * 2
y <- b0 - b1 * x + eps

# hist(eps)
var(eps)
mean(eps)

```

## **POISSON**
```{r poisson_errors}
poisson_small_results <- run_simulations(num_runs, 
                                     n_small, 
                                     num_new_obsv, 
                                     violation =  "Non-normal errors", 
                                     violation_func = "Poisson",
                                     parameters, 
                                     sigma_sq)

poisson_med_results <- run_simulations(num_runs, 
                                   n_med, 
                                   num_new_obsv, 
                                   violation =  "Non-normal errors", 
                                   violation_func = "Poisson", 
                                   parameters, 
                                   sigma_sq)

poisson_large_results <- run_simulations(num_runs, 
                                     n_large, 
                                     num_new_obsv, 
                                     violation =  "Non-normal errors", 
                                     violation_func = "Poisson", 
                                     parameters, 
                                     sigma_sq)

poisson_results <- rbind(small_samp = poisson_small_results, 
                             med_samp = poisson_med_results, 
                             large_samp = poisson_large_results)
poisson_results
```

```{r}
#rpois()
```

3. Heterogeneous Variances
    a.	Simulate errors from a variety of different relationships with X (e.g. eps = 2 * sqrt(x))
    b.	Try simulations with a small sample size (e.g. 20), a medium sample size (e.g. n= 100), and a large sample size (e.g. n = 5000). 
    c.	For each simulation, 
        i.	Predict y-hat at several different locations using a confidence interval.
        ii.	Predict the beta coefficients for a linear model using a confidence interval. 
        iii. Find the MSE (to estimate sigma^2-- does that even make sense here?)
        iv. Test to see whether the beta(s) are significant (t-tests)
    d. Which of the above tasks were affected by the violation of assumptions?
    e. After you have experimented with the effects of different model structures, true parameter values, and sample sizes, let's repeat the simulation but test yourself to see whether you can detect heteroskedacity.
        i. Have R randomly choose whether to simulate errors with constant or non-constant variance
        ii. Simulate data accordingly and display informal/ formal diagnostics as appropriate.
        iii. Based on the diagnostics, predict whether the problem areas you mentioned in part d will be affected or not. (Note: You are not predicting whether the assumptions are violated-- just whether they are violated to such an extent that your ability to use the model is compromised)
        
```{r}
# Sample code to get started
n <- 200
b0 <- 10
b1 <- 10

x <- runif(n, 0, 10)
eps <- rnorm(n, sd = 0.5 * x^2)
y <- b0 + b1 * x + eps

```

```{r}
hetero_var_small_results <- run_simulations(num_runs,
                                     n_small, 
                                     num_new_obsv, 
                                     violation =  "Heterogeneous variances", 
                                     violation_func = poly_func, 
                                     parameters, 
                                     sigma_sq)

hetero_var_med_results <-run_simulations(num_runs, 
                                  n_med, 
                                  num_new_obsv, 
                                  violation =  "Heterogeneous variances",
                                  violation_func = poly_func, 
                                  parameters, 
                                  sigma_sq)

hetero_var_large_results <-run_simulations(num_runs,
                                    n_large, 
                                    num_new_obsv, 
                                    violation =  "Heterogeneous variances", 
                                    violation_func = poly_func, 
                                    parameters, 
                                    sigma_sq)

hetero_var_results <- rbind(small_samp = hetero_var_small_results,
                             med_samp = hetero_var_med_results, 
                             large_samp = hetero_var_large_results)
hetero_var_results
```


4. Correlated Errors
    a.	Simulate errors from a variety of different correlation structures.
    b.	Try simulations with a small sample size (e.g. 20), a medium sample size (e.g. n= 100), and a large sample size (e.g. n = 5000). 
    c.	For each simulation, 
        i.	Predict y-hat at several different locations using a confidence interval.
        ii.	Predict the beta coefficients for a linear model using a confidence interval. 
        iii. Find the MSE (to estimate sigma^2)
        iv. Test to see whether the beta(s) are significant (t-tests)
    d. Which of the above tasks were affected by the violation of assumptions?
    e. After you have experimented with the effects of different model structures, true parameter values, and sample sizes, let's repeat the simulation but test yourself to see whether you can detect correlated errors.
        i. Have R randomly choose whether to simulate data with correlated or uncorrelated errors.
        ii. Simulate data accordingly and display informal/ formal diagnostics as appropriate.
        iii. Based on the diagnostics, predict whether the problem areas you mentioned in part d will be affected or not. (Note: You are not predicting whether the assumptions are violated-- just whether they are violated to such an extent that your ability to use the model is compromised)

```{r}
# Sample code to get started
n <- 200
b0 <- 10
b1 <- 10
rho <- 0.9
sigma <- 2

x <- runif(n, 0, 10)

eps <- arima.sim(model = list(ar = rho), n = n)

y <- b0 + b1 * x + eps

# OR...

eps <- rep(0, n)
e.ind <- rnorm(n, mean = 0, sd = (sigma / sqrt(1-rho^2)))
eps[1] <- e.ind[1]
for (i in 2:n) {
  eps[i] <- rho * eps[i-1] + e.ind[i]
}

y <- b0 + b1 * x + eps
```
        
5. Multicollinearity
    a.	Simulate predictors that are correlated with a variety of different correlation structures
    b.	Try simulations with a small sample size (e.g. 20), a medium sample size (e.g. n= 100), and a large sample size (e.g. n = 5000). 
    c.	For each simulation, 
        i.	Predict y-hat at several different locations using a confidence interval.
        ii.	Predict the beta coefficients for a linear model using a confidence interval. 
        iii. Find the MSE (to estimate sigma^2)
        iv. Test to see whether the beta(s) are significant (t-tests)
    d. Which of the above tasks were affected by the violation of assumptions?
    e. After you have experimented with the effects of different model structures, true parameter values, and sample sizes, let's repeat the simulation but test yourself to see whether you can detect collinearity.
        i. Have R randomly choose whether to simulate data with correlated or uncorrelated predictor variables (X).
        ii. Simulate data accordingly and display informal/ formal diagnostics as appropriate.
        iii. Based on the diagnostics, predict whether the problem areas you mentioned in part d will be affected or not. (Note: You are not predicting whether the assumptions are violated-- just whether they are violated to such an extent that your ability to use the model is compromised)

```{r}
n <- 20
b0 <- 10
b1 <- 3
b2 <- 7

simga <- 2

x1 <- runif(n, 0, 10)
x2 <- x1 + rnorm(n)
cor(x1, x2)


eps <- rnorm(n, sd = sigma)
y <- b0 + b1 * x1 + b2 * x2 + eps





# B <- 100
# many.sims.multicollinearity <- sapply(1:B, function(i) {
#     # x <- rnorm(100, mean = 0, sd = 1)
#     # eps <- rnorm(100, mean = 0, sd = sqrt(0.25))
#     # y <- -1 + 0.5 * x + eps
#     # simul.fit <- lm(y ~ x)
#     # beta.1 <- 0.5
#     # lower <- confint(simul.fit, "x")[1]
#     # upper <- confint(simul.fit, "x")[2]
#     # boolean <- beta.1 <= upper & beta.1 >= lower
#     #return(boolean)
# })

# proportion <- sum(many.sims.multicollinearity)/length(many.sims.multicollinearity)
# proportion

```

<!-- r proportion % of my confidence intervals capture the true value of  .    -->

        
6. Put it all together: Combine the code from the previous 5 parts. Have R randomly choose whether to generate data that violates one (or more) of the assumptions, or whether all the assumptions are valid. Show appropriate diagnostics and test yourself to see if you can predict whether there are problem areas or not. Repeat the simulation several times and record your accuracy at detecting the different problem areas.  


```{r}



```

